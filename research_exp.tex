\documentclass[12pt]{article}

\input{nsf_preamble.tex}


\begin{document}


My previous research has both whetted my appetite for more and humbled me.  I have experienced first-hand that research is not only rewarding and exciting but also challenging, and I have a appreciation of the difficulty of formulating research questions that are both interesting and tractable.  As an undergraduate at Williams College, I worked with Safa Zaki on the cognitive psychology of categorization, and also did a Cognitive Science honors thesis, on computational modeling of speech perception.  As a Baggett Fellow at the University of Maryland, I worked on Bayesian models of unsupervised learning of phonetic categories, and  %%% FIX

Most recently, during my Baggett Fellowship at the University of Maryland, I developed and investigated a sparse-coding model of how the primary auditory cortex might be adapted to the statistics of natural sounds, including speech.  The first phase of my Plan of Research is based on this work.  Sparse coding is a technique that, like principal component analysis, finds an optimal linear transformation between an uninformative way of representing stimuli to a more informative way.  In the example of visual images, each image can be represented by a vector, where each dimension represents the brightness of one image pixel.  However, considering a large set of images, the brightness of one pixel is highly dependent on the brightness of neighboring pixels, and so there is redundancy in this representation.  Linear statistical models remove this redundancy by finding a set of components (which can also be represented as images), such that each original image is represented by the weighted sum of components.  Sparse coding finds the set of components that maximizes the \emph{sparseness} of these sums, or minimizes the number of components that are used to represent each individual image.  As it turns out, the sparse components of natural images are indistinguishable from the receptive fields of single neurons in primary visual cortex (V1).  Each V1 unit's receptive field describes that unit's preferred stimulus, whose presence best predicts a spike by that neuron.

While sparse coding was created as a model of visual perception, the same statistical techniques are equally applicable to any multidimensional stimulus, including sound.  My advisor, Prof. William Idsardi, and I were interested in finding informative features of speech sounds in an unsupervised way, which sparse coding has the potential to do.  Also, after a review of the relevant literature, I discovered that very little attention had been given to such statistical models of auditory perception.  I applied a sparse coding model to speech sounds that had been filtered by a cochlear model (simulating the input that the primary auditory cortex---A1---receives).  I compared the components learned by the model with actual A1 receptive fields, and found that they were very similar, both in terms of frequency, bandwidth, and rate selectivity but also in terms of temporal symmetry, an extremely restrictive property of A1 receptive fields, most of which have essentially the same time-varying impulse response at every frequency band.  These results (especially the similar degrees of temporal symmetry) show that the auditory system, like the visual system, is adapted to the statistics of natural scenes.  This adaptation could happen over evolution, development, or even short-term learning, and further work is necessary to distinguish these possibilities.

While at Maryland I was also involved, to varying degrees, in a variety of other projects.  I ran subjects in an MEG study of the difference between the perception of certain musical intervals embedded in speech and non-speech sounds, and I designed software to automate the pre-processing of video stimuli for a study on the effect on sign-language intelligibility of reversing small, local segments of videos, while preserving the global ordering of these segments.  I also worked with Prof. Jordan Boyd-Graber on the development of a hierarchical Bayesian model which segments a string of phonetic segments in order to learn words and phonetic categories.

The first major component of my undergraduate research experience was a Cognitive Science honors thesis, which was advised by Prof. Safa Zaki, and formulated with the help of Prof. Robert Port at Indiana University over the summer of 2008, (supported by the Tyng Scholarship from Williams).  This project investigated the role that speech-sound representations of various sizes play in the processing of spoken words and non-words.  Having many common subsequences slows the processing of words (since those words tend to sound like many other words, which compete for activation) but speeds the processing of nonwords.  By comparing the range of qualitative patterns of behavior different models were capable of producing, my thesis concluded that large, sublexical representations were necessary to explain the behavioral data (at least within the modeling framework I used).

This result validates, using explicit computational modeling, the explanation that had been given at a theoretical level in the discussion of the behavioral data.  I learned how challenging it is to make such a connection between behavioral data and explicit computational modeling, even when the general framework has already been proposed at a high level.  Implementing and evaluating the models was extremely challenging, and I furthermore had to integrate this technically demanding work into a larger theoretical context.  Prof. Zaki, who advised the project, was instrumental in this, despite not being an expert on either the model I used or the sort of data I was applying it to.  I thus learned the importance of getting input from non-specialists in focusing computational projects and making them broadly relevant.

%%% NEEDS TO BE TIGHTENED UP!
%This project required that I independently implement a number of different models, which required configuring a large number of parameters by hand in order to get reasonable performance on the task I was modeling.  I thus have first-hand knowledge of how frustrating and technically challenging a modeling project can be, not to mention the conceptual difficulties that come with linking modeling results to behavioral ones, but also how rewarding it is to bring such a large and complicated project to completion.  I also learned how important it is to get input from non-specialists, since Dr. Zaki, who advised the project, is not intimately familiar with either the behavioral paradigm or the model I used, and consequently pushed me to clearly explain the larger context and justification for the project.


%Based on analysis of neural network models, this project argued in favor of a role for sublexical representations in the processing of spoken words and nonwords.  Vitevitch and Luce (1999) found that, while phonotactic probability and lexical neighborhood density (LND) strongly covary, they have opposite effects on the processing of words and nonwords: high phonotactic probability facilitates the processing of nonwords, while high LND inhibits the processing of words.  They propose a modeling account which incorporates phonotactic probability and LND effects through different functional pathways that are nevertheless the result of a single learning algorithm.  Critically, in this account, phonotactic probability effects on nonword processing are mediated by relatively large sublexical items (e.g., biphones or syllables) that have the same representational status as lexical items.  While the suitability of this model in this case can be interpreted as indirect evidence for a role for large sublexical representations in speech perception, this argument is undermined by the considerable flexibility of the model.  However, by comparing the a priori qualitative range of behavior of models incorporating different representational schemes, I showed that removing the sublexical representations from the model causes it to behave, in general, in a way that is inconsistent with human performance, lending strength to the case for sublexical representations.

As an undergraduate, I also worked with Prof. Zaki on her projects on the cognitive psychology of categorization, starting in the summer of 2006 when I was hired as a research assistant until I graduated in 2009.  We worked closely together to conceptualize and design studies, and I had primary responsibility for implementing and running.  I also was responsible for the implementation and application of non-standard computational models for analyzing the data we collected.  We're currently writing up two of these projects for publication.

The first was an investigation of the source of apparent dissociation between simple, easily-verbalizable categorization tasks and more complicated tasks that require the integration of information from multiple stimulus dimensions for optimal performance.  Dissociations between these tasks have been used to argue for the existence of multiple, functionally and neurally distinct categorization systems, but there the particular study we were investigating had confounded type of task with different types of difficulty.  With Dr. Zaki, I designed, ran, and analyzed data from a study which removed the dissociation by removing one of these confounds, and we're currently preparing this manuscript for submission.  

The second project we worked on, which is still ongoing, was on the role of transformational knowledge on categorization.  Previous work has shown that subjects are sensitive to coherent transformations associated with categories, and that presenting the same category exemplars in random order vs. as a coherent transformation changes subject's categorization judgements about novel test stimuli.  As currently formulated, most modern models of categorization do not predict and cannot account for these results.  We designed, ran, and analyzed a study which replicated and extended this work, and are currently investigating modeling frameworks which may be able to explain our results.  





\end{document}