\documentclass[12pt]{article}

\input{nsf_preamble.tex}

\begin{document}

%Research plan
%CLEAR STATEMENT OF THE PROBLEM
%-- speech perception - convert continuous/graded acoustic cues into discrete linguistic reprns
Speech perception is the process by which a continuous, acoustic signal---the pressure waves produced by a speaker---is converted into a discrete, linguistic representation---words, sentences, and ultimately meanings.  The fact that state-of-the-art automatic speech recognition systems perform as well as a four-year-old human, and only after extensive training and in tightly proscribed situation, demonstrates that this is a task of extreme complexity which normal humans accomplish with remarkable ease.  Naturally, speech perception has intrigued and challenged psychologists, linguists, and engineers for as long as it has been remotely tractable.  The key to understanding this process is to develop an understanding of the way that speech sounds are represented at the point where they are categorized in a linguistically-relevant way.  I propose a framework for such a model that is based on well-known principles of efficient coding (which well-describe the functioning of early perceptual areas in the brain) and optimal cue-combination (which have been used to explain how different sources of information are combined to make inferences about underlying causes.

%% REPLACED W/ BROAD STATEMENT OF THESIS' GOALS
%However, despite the intense scrutiny by so many researchers, there is currently no single theoretical perspective that is satisfying to all interested parties.  Perhaps due to the difficulty of the problem, researchers in any one field tend to focus on the aspect of speech perception that their field finds relevant and tractable: automatic speech recognition engineers use brute-force statistical methods to infer phoneme labels on small bits of sound.  Basic questions about the features of the acoustic signal are used to distinguish different categories of speech sounds and how those categories are represented in the brain remain unanswered.

For some types of phonetic distinctions, phoneticians have identified relevant acoustic cues.  For instance, \emph{peach} and \emph{beach} differ only in their the voicing of their initial sounds---/p/ is called voiceless and /b/ voiced.  The primary cue to voicing is an acoustic feature known as Voice-Onset Time (VOT), which is the amount of time between the opening of the lips 
%(as indicated by a short burst of broadband noise) 
and the beginning of voicing
%(indicated by the start of regular vibration from the vocal cords)
.  As VOT increases, perception switches from a /b/ percept to a /p/ abruptly at around 40 ms VOT.

% IS THIS NECESSARY? FLORIAN SEEMED TO LIKE IT SO...
Based on the existence of simple cues such as VOT, it would be natural to think that phonetic category identification should be as simple as reading off a few acoustic measures, but in practice this simple picture is complicated by two facts.  The first is that, due to systematic and random variation, categories are actually realized as overlapping distributions on any given cue dimension \cite{Hillenbrand1995}.  The second is that, in order to deal with the ambiguous nature of these cues, listeners combine information from a large number of cues in order to make phonetic judgements, and almost every cue is used in making multiple phonetic distinctions.  

%Any theory of phonetic categories must, at a minimum, explain how information form multiple acoustic cue dimensions is combined, and what the nature of these acoustic is.  Statistical models provide substantial insight for both of these problems, and they do it in a way that is domain-general, but which still allows language-specific representations to be learned through language-specific interactions between different levels of representations.  

Furthermore, it's not clear that the acoustic cues identified so far are the ones actually used in human speech perception.  My proposed model thus has two parts.  The first part models how the auditory system extracts informative acoustic cue dimensions by adapting to the statistical structure of complex natural stimuli.  The second part models how information from these cues is combined to form phonetic categories.  Both of these models are domain-general statistical models of perception and categorization, supported by rigorous mathematics, and have been used to explain a variety of behavioral and neural data.

The first phase of my project is learning informative acoustic cue dimensions using variants of sparse coding.  My preliminary data\cite{Kleinschmidt2010} (as discussed in Previous Research Experience) has demonstrated that this approach learns representations that match those used by the primary auditory cortex quite well, based both speech and non-speech natural sounds.

%Sparse coding \cite{Olshausen1996} has proven to be a robust description of perceptual processing at various levels, including the primary visual cortex (V1) and the cochlea.  Sparse coding learns a set of basis functions for its input such that for each piece of input, as few basis functions are necessary are used to reconstruct it.  When applied to images taken from natural scenes, sparse coding learns basis functions that are functionally equivalent to the receptive fields of single neurons in V1.  

The second phase will be the development of a rational model of how the different acoustic cues learned by the first phase can be best combined to effectively make phonetic distinctions.  Work on rational cue-combination has shown that the best way to make a decision based on multiple cues is to pay more attention to more reliable cues.  Such models have already been used to explain the trading relationships between VOT, vowel length, and formant frequency in determining voicing \cite{Toscano2008}.  These models simultaneously learn the underlying number of categories and the optimal way of combining different cues.  I plan to extend such models to learn the optimal weighting from the acoustic features extracted from speech in the first phase to known phonetic distinctions (e.g. voicing).

I will evaluate this model using the same behavioral dataset as \cite{Toscano2008}, training the model on examples of voiced and voiceless consonants extracted from actual speech and represented using the features used in phase one.  Using a common dataset allows a direct comparison between the model which uses learned representations and a model---with the same learning rule---which uses features identified by phoneticians as being relevant to voicing, and is a strong test of the ability of the learned features in phase one to support speech perception.

The third phase will investigate the effect of allowing these different learning processes to interact.  While the features represented in A1 may be genetically specified, it is possible that they are learned during development, and learning phonetic categories simultaneously may change the nature of these low-level features.  Both stages of the model described so far are statistical models, and they can be combined into a single, two-stage model which simultaneously learns acoustic features, phonetic categories, and the weighting between them.  In order to characterize the top-down effect of learning phonetic categories, the acoustic representation learned by such a hybrid model can be compared to those learned by the phase one model alone.

%Since rational cue-combination models have already been applied to phonetic categorization using traditional phonetic features like VOT, this second phase constitutes a good test of the usefulness of the acoustic cues identified in the first phase.  

% this is probably too much for this proposal, but maybe I can find a concise way of explaining it...
%Currently, cue-combination models have been used to explain how the proper number of categories can be learned on a \emph{given} phonetic dimension.  However, part of the process of language learning is learning which phonetic dimensions are relevant in the learner's native language.  Statistical models exist which can model this process, as well.

%The third phase is to investigate how learning at one level might influence learning at other levels.  For instance, significant, task-mediated plasticity in A1 receptive fields has been demonstrated which persis
%The third phase of the project will be an investigation of the interaction between these two stages.  There is evidence that manipulating the distribution of cues changes perception and categorization behavior, and moreover that task demands can lead to short- and long-term changes in the receptive fields of individual neurons.  Moreover, work by Kraljic, Samuels, and others has shown that listeners can adapt their phonetic categories online in specific ways, exhibiting varying degrees of generalization.  A model where the sparse-coded representations of acoustic stimuli interacts with an optimal cue-combination process has the potential to elucidate how, why, and at what level these effects occur.

% DISCUSS THIS W/ FLORIAN
%Since both stages of the model described thus far ultimately describe distributions over acoustic cues, they can be combined to produce a single multi-stage model which links acoustic observables with latent phonetic categories.  There are a number of ways such a multi-stage model is useful, but in the interest of space I will describe only one here.  A substantial amount of evidence suggests that listeners very quickly modify the boundaries between phonetic categories when faced with speech that is accented or otherwise peculiar (cite Kraljik etc.).  This can be modeled in a 




%% BEHAVIORAL EXTENSION?
% I think I can roll this in with other stuff above...maybe?
%The statistical models that I have proposed embody specific hypotheses about what types of acoustic information is relevant to speech perception and thus make specific behavioral predictions.  By re-coding speech from a raw acoustic form into dimensions corresponding to the features learned by the model, and comparing different phonetic categories on these dimensions, it is possible to predict the effects of manipulating certain channels on the intelligibility of speech or the discriminability of certain contrasts, which can be measured using standard psycholinguistics paradigms such as visual world eye-tracking.  Just as VOT (whose importance was determined by manual inspection of lots of phonetic data) is a good predictor of whether a bilabial stop will be heard as a /p/ or a /b/, so particular combinations of modeled features should be useful in making different phonetic contrasts.

The intellectual merit of this approach to modeling and understanding human speech perception lies in the confluence of methods and results linguistics, neuroscience, psychology, and machine learning.  The models that I have proposed are general statistical techniques that have been applied broadly, but which are generally consistent with the current neural and behavioral data on speech perception, as well as being plausible from broader neural, developmental, and behavioral perspectives.

The broader impacts of this project are twofold.  First, because it brings together insights and methods from many fields, it will require broad collaboration and dissemination across disciplines.  To language scientists, it potentially offers a way of integrating speech perception into the broader framework of cognitive science.  For researchers in neuroscience, machine learning, and general cognitive psychology, speech perception can serve as a challenging model domain for their methods and theories.  Second insights gleaned from this work on how the auditory system underlies speech perception can inform the development of better cochlear implants, and have potential to advance automatic speech recognition technology as well. 

%Such a massively interdisciplinary project necessarily has a very broad impact.  In the same way that work on speech perception from a number of fields provides substantial constraints on any tenable model of speech perception, so does such a model provide a range of testable hypotheses about the neural, developmental, behavioral, and computational traces of speech perception.  There are also obvious practical applications.  Automatic speech recognition is a challenging technological problem that human beings solve well, and so any insight into how human speech perception works has the potential to improve ASR technology.  
% More importantly, COCHLEAR IMPLANTS BLAH BLAH BLAH

%\bibliographystyle{apacitex}
\bibliographystyle{nsf}
{
\fontsize{10}{10}
\selectfont
\bibliography{nsf}
}

\end{document}
