\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{fontspec}
\setmainfont[Mapping=tex-text]{Times New Roman}

%\usepackage{apacite}
\usepackage{cite}

\begin{document}

%Research plan
%CLEAR STATEMENT OF THE PROBLEM
%-- speech perception - convert continuous/graded acoustic cues into discrete linguistic reprns
Speech perception is the process by which a continuous, acoustic signal---the pressure waves produced by a speaker---is converted into a discrete, linguistic representation---words, sentences, and ultimately meanings.  The fact that state-of-the-art automatic speech recognition systems perform as well as a four-year-old human, and only after extensive training and in tightly proscribed situation, demonstrates that this is a task of extreme complexity which normal humans accomplish with remarkable ease.  Naturally, speech perception has intrigued and challenged psychologists, linguists, and engineers for as long as it has been remotely tractable.  The key to understanding this process is to develop an understanding of the way that speech sounds are represented at the point where they are categorized in a linguistically-relevant way.  I propose a framework for such a model that is based on well-known principles of efficient coding (which well-describe the functioning of early perceptual areas in the brain) and optimal cue-combination (which have been used to explain how different sources of information are combined to make inferences about underlying causes.

%% REPLACED W/ BROAD STATEMENT OF THESIS' GOALS
%However, despite the intense scrutiny by so many researchers, there is currently no single theoretical perspective that is satisfying to all interested parties.  Perhaps due to the difficulty of the problem, researchers in any one field tend to focus on the aspect of speech perception that their field finds relevant and tractable: automatic speech recognition engineers use brute-force statistical methods to infer phoneme labels on small bits of sound.  Basic questions about the features of the acoustic signal are used to distinguish different categories of speech sounds and how those categories are represented in the brain remain unanswered.

For some types of phonetic distinctions, phoneticians have identified relevant acoustic cues.  For instance, \emph{peach} and \emph{beach} differ only in their the voicing of their initial sounds---/p/ is called voiceless and /b/ voiced.  The primary cue to voicing is an acoustic feature known as Voice-Onset Time (VOT), which is the amount of time between the opening of the lips 
%(as indicated by a short burst of broadband noise) 
and the beginning of voicing 
%(indicated by the start of regular vibration from the vocal cords)
.  As VOT increases, perception switches from a /b/ percept to a /p/ abruptly at around 40 ms VOT.

% IS THIS NECESSARY? FLORIAN SEEMED TO LIKE IT SO...
Based on the existence of such cues, it would be natural to think that phonetic category identification should be as simple as reading off a few acoustic measures, but in practice this simple picture is complicated by two facts.  The first is that, due to systematic and random variation, categories map to overlapping distributions of any given set of cues \cite{Hillenbrand1995}.  The second is that, in order to deal with the ambiguous nature of these cues, listeners combine information from a large number of cues in order to make phonetic judgements.
%The first is simply that all of these acoustic cues are gradient, whereas the underlying phonetic categories are discrete.  The second is that there is not a one-to-one mapping between acoustic cues and phonetic categories: each category has many cues, and each cue is relevant to many categorical distinctions.  

% shorten/eliminate this?  put in stuff at the end about perceptual learning (e.g. Kraljic & Samuels) -- do learned representations exhibit same sort of plasticity?  advantage of probabilistic/distributional framework is that it allows other dimensions to modulate judgements along a given dimension in a principled way --- toscano & mcmurray (2008 cog sci) --- florian: "there is evidence that features can be down-weighted (see vroomen et al's 07 paper for a summary of work on "perceptual recalibration"; also pado and ramirez 06). but very little of that work has taken the idea of optimal cue integration seriously / done is justice (kraljic and samuels, 06, 07, 08, 09 maybe also be relevant, though less directly so)."
Phonetics has traditionally regarded each phonetic category as being represented as a single, canonical set of acoustic cues, but there is a growing body of work which suggests that each phonetic category is instead better understood as a distribution over a set of acoustic features.  Such an extension has the advantage of being a minimal extension of the traditional view---since these distributions can be summarized by e.g. their mean values---but which also acounts for evidence that both adults and children are sensitive to the distribution of cues encountered in an experimental setting.  


Any theory of phonetic categories must, at a minimum, explain how information form multiple acoustic cue dimensions is combined, and what the nature of these acoustic is.  Statistical models provide substantial insight for both of these problems, and they do it in a way that is domain-general, but which still allows language-specific representations to be learned through language-specific interactions between different levels of representations.  

My proposed model thus has two parts.  The first part models how the auditory cortex captures the statistical structure of a complex acoustic environment in order to extract informative acoustic cue dimensions.  The second part models how information from these cues is combined to form phonetic categories.  Both of these models are domain-general models of perception and categorization, and have been shown to be plausible solutions to a wide variety of issues.

Sparse coding \cite{Olshausen1996} has proven to be a robust description of perceptual processing at various levels, including the primary visual cortex (V1) and the cochlea.  Sparse coding learns a set of basis functions for its input such that for each piece of input, as few basis functions are necessary are used to reconstruct it.  When applied to images taken from natural scenes, sparse coding learns basis functions that are functionally equivalent to the receptive fields of single neurons in V1.  


The second phase will be the development of a rational model of how the different acoustic cues learned by the first phase can be best combined to effectively make phonetic distinctions.  Work on rational-cue combination has shown that the best way to make a decision based on multiple cues is to pay more attention to more reliable cues.  Such models have already been used to explain the trading relationships between VOT, vowel length, and formant frequency in determining voicing \cite{Toscano2008}.  These models learn the underlying number of categories in an unsupervised fashion, and in doing so learn the optimal cue weighting.  I plan to extend such models to learn the optimal weighting from acoustic features learned in the first phase to known phonetic distinctions (e.g. voicing) using cue values measured from actual speech.  
% this is probably a little too vague?
Ultimately, I plan to develop a hierarchical extension of this model which additionally infers the underlying phonetic dimensions (e.g. the \emph{voicing} feature).

Since rational cue-combination models have already been applied to phonetic categorization using traditional phonetic features like VOT, this second phase constitutes a good test of the usefulness of the acoustic cues identified in the first phase.  


%% BEHAVIORAL EXTENSION?
% I think I can roll this in with other stuff above...maybe?
The statistical models that I will use can be thought of as embodying specific hypotheses about what types of acoustic information is relevant to speech perception and thus make specific behavioral predictions.  By re-coding speech from a raw acoustic form into dimensions corresponding to the features learned by the model, and comparing different phonetic categories on these dimensions, it is possible to predict the effects of manipulating certain channels on the intelligibility of speech or the discriminability of certain contrasts, which can be measured using standard psycholinguistics paradigms such as visual world eye-tracking.  Just as VOT (whose importance was determined by manual inspection of lots of phonetic data) is a good predictor of whether a bilabial stop will be heard as a /p/ or a /b/, so particular combinations of modeled features should be useful in making different phonetic contrasts.

The intellectual merit of this approach to modeling and understanding human speech perception lies in the confluence of linguistics, neuroscience, psychology, and machine learning methods and results.  The models that I have proposed are general statistical techniques that have seen wide-ranging application, but which are consistent with the broad outlines of what is currently known about speech perception, as well as being neurally, developmentally, and behaviorally plausible.

Such a massively interdisciplinary project necessarily has a very broad impact.  In the same way that work on speech perception from a number of fields provides substantial constraints on any tenable model of speech perception, so does such a model provide a range of testable hypotheses about the neural, developmental, behavioral, and computational traces of speech perception.  There are also obvious practical applications.  Automatic speech recognition is a challenging technological problem that human beings solve well, and so any insight into how human speech perception works has the potential to improve ASR technology.  
% More importantly, COCHLEAR IMPLANTS BLAH BLAH BLAH

%\bibliographystyle{apacitex}
\bibliographystyle{plain}
\bibliography{nsf}

\end{document}
